{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a third model I trained using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the pretrained VGG16 model, and although it already has a category \"runnning shoe\" I still wanted to implement this as an experimanet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization, regularizers, MaxPooling2D, Conv2D\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'models/transfer_learning_model/top_model_weights.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2449 * 2\n",
    "nb_validation_samples = 270 * 2\n",
    "epochs = 50\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 540 images belonging to 2 classes.\n",
      "27/27 [==============================] - 193s 7s/step\n"
     ]
    }
   ],
   "source": [
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # Build the VGG16 network\n",
    "    model = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    '''# Generate training images\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)\n",
    "    \n",
    "    # Save training weights\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size ,  verbose=1)\n",
    "    np.save(\"models/transfer_learning_model/bottleneck_features_train2.npy\", bottleneck_features_train)'''\n",
    "\n",
    "    # Generate validation images\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    # Save training weights\n",
    "    bottleneck_features_validation = model.predict_generator( generator, nb_validation_samples // batch_size,  verbose=1)\n",
    "    np.save(\"models/transfer_learning_model/bottleneck_features_validation2.npy\", bottleneck_features_validation)\n",
    "\n",
    "### save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 4, 4, 512)\n",
      "Train on 4896 samples, validate on 540 samples\n",
      "Epoch 1/50\n",
      "4896/4896 [==============================] - 19s 4ms/step - loss: 0.1845 - acc: 0.9461 - val_loss: 0.0481 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98148, saving model to models/transfer_learning_model/weights-improvement-01-0.98.hdf5\n",
      "Epoch 2/50\n",
      "4896/4896 [==============================] - 14s 3ms/step - loss: 0.0878 - acc: 0.9773 - val_loss: 0.0541 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98148 to 0.98333, saving model to models/transfer_learning_model/weights-improvement-02-0.98.hdf5\n",
      "Epoch 3/50\n",
      "4896/4896 [==============================] - 15s 3ms/step - loss: 0.0641 - acc: 0.9843 - val_loss: 0.0847 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.98333\n",
      "Epoch 4/50\n",
      "4896/4896 [==============================] - 14s 3ms/step - loss: 0.0489 - acc: 0.9882 - val_loss: 0.0773 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98333 to 0.98519, saving model to models/transfer_learning_model/weights-improvement-04-0.99.hdf5\n",
      "Epoch 5/50\n",
      "4896/4896 [==============================] - 15s 3ms/step - loss: 0.0479 - acc: 0.9894 - val_loss: 0.0864 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.98519\n",
      "Epoch 6/50\n",
      "4896/4896 [==============================] - 14s 3ms/step - loss: 0.0350 - acc: 0.9912 - val_loss: 0.2175 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.98519\n",
      "Epoch 7/50\n",
      "4896/4896 [==============================] - 15s 3ms/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.1226 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98519\n",
      "Epoch 8/50\n",
      "4896/4896 [==============================] - 14s 3ms/step - loss: 0.0242 - acc: 0.9935 - val_loss: 0.1162 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98519\n",
      "Epoch 9/50\n",
      "4896/4896 [==============================] - 15s 3ms/step - loss: 0.0181 - acc: 0.9957 - val_loss: 0.1331 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98519\n",
      "Epoch 10/50\n",
      "4896/4896 [==============================] - 15s 3ms/step - loss: 0.0213 - acc: 0.9941 - val_loss: 0.1227 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98519\n",
      "Epoch 11/50\n",
      "4896/4896 [==============================] - 15s 3ms/step - loss: 0.0183 - acc: 0.9965 - val_loss: 0.1346 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98519\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Create training labels and load training data\n",
    "train_data = np.load('models/transfer_learning_model/bottleneck_features_train2.npy')\n",
    "\n",
    "train_labels_0 = np.zeros(int(4896 /2))\n",
    "train_labels_1 = np.ones(int(4896 /2))\n",
    "train_labels = np.concatenate((train_labels_0, train_labels_1), axis=0)\n",
    "\n",
    "# Create validation labels and load validation data\n",
    "validation_data = np.load('models/transfer_learning_model/bottleneck_features_validation2.npy')\n",
    "print(validation_data.shape)\n",
    "validation_labels_0 = np.zeros(int(540/2))\n",
    "validation_labels_1 = np.ones(int(540/2))\n",
    "validation_labels = np.concatenate((validation_labels_0, validation_labels_1), axis=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "filepath = \"models/transfer_learning_model/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "csv_logger = CSVLogger('models/transfer_learning_model/training_tf_log.csv')\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, csv_logger]\n",
    "\n",
    "validation_labels = validation_labels.reshape(540, 1)\n",
    "history = model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=( validation_data , validation_labels), \n",
    "          callbacks=callbacks_list)\n",
    "model.save_weights(top_model_weights_path)\n",
    "model.save('models/transfer_learning_model/top_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TFEnv] *",
   "language": "python",
   "name": "conda-env-TFEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
